<?xml version="1.0" encoding="UTF-8"?>
<section xmlns="http://docbook.org/ns/docbook" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" version="5.0" xml:id="section_cinder-deployment-choices">
    <title>Theory of Operation &amp; Deployment Choices</title>
    <simplesect>
        <title>Construct Mappings between Cinder and Data ONTAP</title>
        <formalpara>
            <title>Cinder Backends and Storage Virtual Machines</title>
            <para>
                 Storage Virtual Machines (SVMs, formerly known as Vservers) contain data volumes and one or more LIFs through which they serve data to clients. SVMs can either contain one or more FlexVol volumes, or a single Infinite Volume. 
            </para>
        </formalpara>
        <para>
            SVMs securely isolate the shared virtualized data storage and network, and each SVM appears as a single dedicated storage virtual machine to clients. Each SVM has a separate administrator authentication domain and can be managed independently by its SVM administrator.
        </para>
        <para>
            In a cluster, SVMs facilitate data access. A cluster must have at least one SVM to serve data. SVMs use the storage and network resources of the cluster. However, the volumes and LIFs are exclusive to the SVM. Multiple SVMs can coexist in a single cluster without being bound to any node in a cluster. However, they are bound to the physical cluster on which they exist.
        </para>
        <important>
            <para>
                When deploying Cinder with clustered Data ONTAP, the recommended mapping is to have each Cinder backend refer to a single SVM within a cluster using the <option>netapp_vserver</option> configuration option. While the driver can operate without the explicit declaration of a mapping between a backend and an SVM, a variety of advanced functionality (e.g. volume type extra-specs) will be disabled.
            </para>
        </important>
        <formalpara>
            <title>Cinder volumes and FlexVol volumes</title>
            <para>
                Data ONTAP FlexVol volumes (commonly referred to as volumes) and OpenStack Block Storage volumes (commonly referred to as Cinder volumes) are not semantically analogous. A FlexVol volume is a container of logical data elements (for example: files, Snapshot copies, clones, LUNs, et cetera) that is abstracted from physical elements (for example: individual disks, and RAID groups). A Cinder volume is a block device. Most commonly, these block devices are made available to OpenStack Compute instances. NetApp’s various driver options for deployment of FAS as a provider of Cinder storage place Cinder volumes, snapshots copies, and clones within FlexVol volumes. 
            </para>
            <important>
                <para>
                    The FlexVol volume is an overarching container for one or more Cinder volumes.
                </para>
            </important>
        </formalpara>
        <formalpara>
            <title>Cinder volume representation within a FlexVol volume</title>
            <para>
                A Cinder volume has a different representation in Data ONTAP when stored in a FlexVol volume, dependent on storage protocol utilized with Cinder:
            </para>
            <itemizedlist>
                <listitem><emphasis>iSCSI</emphasis>: When utilizing the iSCSI storage protocol, a Cinder volume is stored as an iSCSI LUN.</listitem>
                <listitem><emphasis>NFS</emphasis>: When utilizing the NFS storage protocol, a Cinder volume is a file on an NFS export.</listitem>
            </itemizedlist>
        </formalpara>
        <formalpara>
            <title>Cinder snapshots versus NetApp snapshots</title>
            <para>
                A NetApp snapshot copy is a point-in-time file system image. Low-overhead Snapshot copies are made possible by the unique features of the WAFL storage virtualization technology that is part of Data ONTAP. Up to 255 Snapshot copies can kept at any time for a FlexVol volume.
            </para>
        </formalpara>
        <para>
             Since NetApp Snapshots are taken at the FlexVol level, they can not be directly leveraged within an OpenStack context, as a user of Cinder requests a snapshot be taken of a particular Cinder volume (not the containing FlexVol volume). As a Cinder volume is represented as either a file on NFS or an iSCSI LUN, the way that Cinder snapshots are created is through use of NetApp's FlexClone technology.  
        </para>
        <para>
            FlexClone files or FlexClone LUNs and their parent files or LUNs that are present in the FlexClone volume continue to share blocks the same way they do in the parent FlexVol volume. In fact, all the FlexClone entities and their parents share the same underlying physical data blocks, minimizing physical disk space usage. 
        </para>
        <important>
            <para>
                When Cinder is deployed with Data ONTAP, Cinder snapshots are created leveraging the FlexClone feature of Data ONTAP. As such, a license option for FlexClone must be enabled.  
            </para>
        </important>
    </simplesect>
    <simplesect>
        <title>Deployment Choice: Direct versus Intermediated</title>
        <para>
            NetApp's Cinder driver can operate in two independent modes: a <emphasis>direct</emphasis> mode where the Cinder processes directly interact with NetApp FAS storage systems, and an <emphasis>intermediated</emphasis> mode where the Cinder processes interact with an additional software entity that issues provisioning and management requests on behalf of Cinder.
        </para>
        <formalpara>
            <title>OnCommand Workflow Automator</title>
            <para>
                OnCommand Workflow Automator (WFA) is a flexible framework that provides automation for storage-related tasks, customization, scripting capabilities, and integration with higher-order IT systems such as orchestration software through web services.
            </para>
        </formalpara>
        <para>
            While WFA can be utilized in conjunction with NetApp's unified Cinder driver, a deployment of Cinder and WFA does introduce additional complexity, management entities, and potential points of failure within a cloud architecture. If you have an existing set of workflows that are written within the WFA framework, and are looking to leverage them in lieu of the default provisioning behavior of the Cinder driver operating directly against a FAS system, then it may be desirable to use the intermediated mode.
        </para>
        <formalpara>
            <title>SANtricity<trademark class="registered"/> Web Services Proxy</title>
            <para>
                The NetApp SANtricity Web Services Proxy provides access through standard HTTPS mechanisms to configuring management services for E-Series storage arrays. You can install Web Services Proxy on either Linux or Windows. As Web Services Proxy satisfies the client request by collecting data or executing configuration change requests to a target storage array, the Web Services Proxy module issues SYMbol requests to the target storage arrays. Web Services Proxy provides a Representative State Transfer (REST)-style API for managing E-Series controllers. The API enables you to integrate storage array management into other applications or ecosystems.
            </para>
        </formalpara>
        <important>
            <title>Recommendation</title>
            <para>
                Unless you have a significant existing investment with OnCommand Workflow Automator that you wish to leverage in an OpenStack deployment, it is recommended that you start with the <emphasis>direct</emphasis> mode of operation when using Cinder with a NetApp FAS deployment. When using Cinder with a NetApp E-Series deployment, use of the SANtricity Web Services Proxy in the <emphasis>intermediated</emphasis> mode is currently required. 
            </para>
        </important>
    </simplesect>
    <simplesect>
        <title>Deployment Choice: FAS vs E-Series</title>
        <formalpara>
            <title>FAS</title>
            <para>
                If rich data management, deep data protection, and storage efficiency are desired and should be availed directly by the storage, the NetApp FAS product line is a natural fit for use within Cinder deployments. Massive scalability, nondisruptive operations, proven storage efficiencies, and a unified architecture (NAS and SAN) are key features offered by the Data ONTAP storage operating system. These capabilities are frequently leveraged in existing virtualization deployments and thus align naturally to OpenStack use cases.
            </para>
        </formalpara>
        <formalpara>
            <title>E-Series</title>
            <para>
                For cloud environments where higher performance is critical, or where higher-value data management features are not needed or are implemented within an application, NetApp's E-Series product line can provide a cost-effective underpinning for a Cinder deployment. NetApp's E-Series storage offers a feature called Dynamic Disk Pools, which simplifies data protection by removing the complexity of configuring RAID groups and allocating hot spares. Utilization is improved by dynamically spreading data, parity, and spare capacity across all drives in a pool, reducing performance bottlenecks due to hot-spots. Additionally, should a drive failure occur, DDP enables the pool to return to an optimal state significantly faster than RAID6, while reducing the performance impact during the reconstruction of a failed drive. 
            </para>
        </formalpara>
        <note>
            <para>
                As of the Icehouse release, NetApp has integrations with Cinder for both FAS and E-Series, and either storage solution can be included as part of a Cinder deployment to leverage the native benefits that either platform has to offer.
            </para>
        </note>
    </simplesect>
    <simplesect>
        <title>Deployment Choice: Clustered Data ONTAP vs Data ONTAP operating in 7-Mode</title>
        <para>
            Clustered Data ONTAP represents NetApp’s platform for delivering future innovation in the FAS product line. Its inherent qualities of virtualization of network interfaces, disk subsystem, and administrative storage controller map well to OpenStack constructs. The Storage Virtual Machine storage server (SVM, historically referred to as Vserver) can span across all nodes of a given clustered Data ONTAP deployment, for example. The elasticity provided to expand or contract a Storage Virtual Machine across horizontally scalable resources are capabilities critical to cloud deployment unique to the clustered Data ONTAP mode of operation.
        </para>
        <para>
            The Data ONTAP 7-Mode drivers are primarily provided to allow rapid use of prior deployed FAS systems for OpenStack block storage requirements. There is no current intention to enhance the 7-Mode driver’s capabilities beyond providing basic bug fixes.
        </para>
        <important>
            <title>Recommendation</title>
            <para>
                NetApp strongly recommends that all OpenStack deployments built upon NetApp's FAS product set leverage clustered Data ONTAP.
            </para>
        </important>
    </simplesect>
    <simplesect>
        <title>Deployment Choice: NFS versus iSCSI</title>
        <para>
            A frequent question from customers and partners is whether to utilize NFS or iSCSI as the storage protocol with a Cinder deployment ontop of NetApp's FAS product line. 
            Both protocol options are TCP/IP-based, deliver similar throughputs and latencies, support Cinder features, snapshot copies and cloning are supported to similar degrees, as well as advertisement of other storage efficienty, data protection, and high availability features.
        </para>
        <formalpara>
            <title>iSCSI</title>
            <itemizedlist>
                <listitem>At the time of publishing, the maximum number of iSCSI LUNs per NetApp cluster is either 8,192 or 49,152 - dependent on the FAS model number (refer to <link xlink:href="http://hwu.netapp.com">Hardware Universe</link> for detailed information for a particular model). Cinder can be configured to operate with multiple NetApp clusters via multi-backend support to increase this number for an OpenStack deployment.</listitem>
                <listitem>LUNs consume more management resources and some management tools also have limitations on the number of LUNs.</listitem>
                <listitem>When Cinder is used independently of OpenStack Compute, use of iSCSI is essential to provide direct access to block devices. The Cinder driver use in conjunction with NFS relies on libvirt and the hypervisor to represent files on NFS as virtual block devices. When Cinder is utilized in bare-metal or non-virtualized environments, the NFS storage protocol is not an option.</listitem>
            </itemizedlist>
        </formalpara>
        <formalpara>
            <title>NFS</title>
            <itemizedlist>
                <listitem>The maximum number of files in a single FlexVol volume exported through NFS is dependent on the size of the FlexVol volume; a 1TB FlexVol can have 33,554,432 files (assuming 32k inodes). The theoretical maximum of files is roughly two billion.</listitem>
                <listitem>NFS drivers require support from the hypervisor to virtualize files and present them as block devices to an instance.</listitem>
                <!-- refer to CDOT NFS BP guide? http://www.netapp.com/us/system/pdf-reader.aspx?m=tr-4067.pdf -->
            </itemizedlist>
        </formalpara>
        <itemizedlist>
            <listitem>There is no difference in the maximum size of a Cinder volume regardless of the storage protocol chosen (a file on NFS or an iSCSI LUN are both 16TB). </listitem>
            <listitem>Performance differences between iSCSI and NFS are normally negligible in virtualized environments; for a detailed investigation, please refer to <link xlink:href="http://www.netapp.com/us/system/pdf-reader.aspx?m=tr-3808.pdf&amp;cc=us">NetApp TR3808: VMware vSphere and ESX 3.5 Multiprotocol Performance Comparison using FC, iSCSI, and NFS</link>.</listitem>
        </itemizedlist>
        <important>
            <title>Recommendation</title>
            <para>
                Deploying NetApp's Cinder driver with clustered Data ONTAP utilizing the NFS storage protocol yields a more scalable OpenStack deployment than iSCSI with negligible performance differences. If Cinder is being used to provide block storage services independent of other OpenStack services, the iSCSI protocol must be utilized.
            </para>
        </important>
        <tip>
            <para>
                A related use case for using iSCSI with OpenStack deployments involves creating a FlexVol volume to serve as the storage for OpenStack compute nodes. As more hypervisor nodes are added, a master boot LUN can simply be cloned for each node, and compute nodes can become completely stateless. Since the configuration of hypervisor nodes are usually nearly identical (except for node-specific data like configuration files, logs, etc), the boot disk lends well to optimizations like deduplication and compression.
            </para>
            <para>
                Currently this configuration must be done outside of the management scope of Cinder, but it serves as another example of how NetApp's differentiated capabilities can be leveraged to ease the deployment and ongoing operation of an OpenStack cloud deployment.
            </para>
        </tip>
    </simplesect>
    <simplesect xml:id="cinder.volume_types">
        <title>Using Cinder Volume Types to Create a Storage Service Catalog</title>
        <para>
            The Storage Service Catalog (SSC) is a concept that describes a set of capabilities that enables efficient, repeated, and consistent use and management of storage resources by the definition of policy-based services and the mapping of those services to the backend storage technology. It is meant to abstract away the actual technical implementations of the features at a storage backend into a set of simplified configuration options.
        </para>
        <para>
            The storage features are organized or combined into groups based on the customer needs to achieve a particular scenario or use case. Based on the catalog of the storage features, intelligent provisioning decisions are made by infrastructure or software enabling the storage service catalog. In OpenStack, this is achieved together by the Cinder filter scheduler and the NetApp driver by making use of volume type extra-specs support together with the filter scheduler. There are some prominent features which are exposed in the NetApp driver including mirroring, de-duplication, compression, and thin provisioning.
        </para>
        <para>
            When using the NetApp unified driver with a clustered Data ONTAP storage system, you can leverage extra specs with Cinder volume types to ensure that Cinder volumes are created on storage backends that have certain properties (e.g. QoS, mirroring, compression) configured.
        </para>
        <para>
            Extra specs are associated with Cinder volume types, so that when users request volumes of a particular volume type, they will be created on storage backends that meet the list of requirements (e.g. available space, extra specs, etc). You can use the specs in <xref linkend="cinder.netapp.extra_specs"/> later in this section when defining Cinder volume types using the <command>cinder type-key</command> command.
        </para>
        <table rules="all" xml:id="cinder.netapp.extra_specs">
            <caption>NetApp-supported Extra Specs for use with Cinder Volume Types</caption>
            <col width="1.75in"/>
            <col width="0.65in"/>
            <col width="3.35in"/>
            <thead>
                <tr>
                    <td>Extra spec</td>
                    <td>Type</td>
                    <td>Description</td>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td><option>netapp:raid_type</option></td>
                    <td>String</td>
                    <td>Limit the candidate volume list based on one of the following raid types: <option>raid4</option>, <option>raid_dp</option>.</td>
                </tr>
                <tr>
                    <td><option>netapp:disk_type</option></td>
                    <td>String</td>
                    <td>Limit the candidate volume list based on one of the following disk types: <option>ATA</option>, <option>BSAS</option>, <option>EATA</option>, <option>FCAL</option>, <option>FSAS</option>, <option>LUN</option>, <option>MSATA</option>, <option>SAS</option>, <option>SATA</option>, <option>SCSI</option>, <option>XATA</option>, <option>XSAS</option>, or <option>SSD</option>.</td>
                </tr>
                <tr>
                    <td><option>netapp:qos_policy_group</option></td>
                    <td>String</td>
                    <td>Limit the candidate volume list based on the name of a QoS policy group, which defines measurable Service Level Objectives that apply to the storage objects with which the policy group is associated.</td>
                </tr>
                <tr>
                    <td><option>netapp_mirrored</option></td>
                    <td>Boolean</td>
                    <td>Limit the candidate volume list to only the ones that are mirrored on the storage controller.</td>
                </tr>
                <tr>
                    <td><option>netapp_unmirrored</option></td>
                    <td>Boolean</td>
                    <td>Limit the candidate volume list to only the ones that are not mirrored on the storage controller.</td>
                </tr>
                <tr>
                    <td><option>netapp_dedup</option></td>
                    <td>Boolean</td>
                    <td>Limit the candidate volume list to only the ones that have deduplication enabled on the storage controller.</td>
                </tr>
                <tr>
                    <td><option>netapp_nodedup</option></td>
                    <td>Boolean</td>
                    <td>Limit the candidate volume list to only the ones that have deduplication disabled on the storage controller.</td>
                </tr>
                <tr>
                    <td><option>netapp_compression</option></td>
                    <td>Boolean</td>
                    <td>Limit the candidate volume list to only the ones that have compression enabled on the storage controller.</td>
                </tr>
                <tr>
                    <td><option>netapp_nocompression</option></td>
                    <td>Boolean</td>
                    <td>Limit the candidate volume list to only the ones that have compression disabled on the storage controller.</td>
                </tr>
                <tr>
                    <td><option>netapp_thin_provisioned</option></td>
                    <td>Boolean</td>
                    <td>Limit the candidate volume list to only the ones that support thin provisioning on the storage controller.</td>
                </tr>
                <tr>
                    <td><option>netapp_thick_provisioned</option></td>
                    <td>Boolean</td>
                    <td>Limit the candidate volume list to only the ones that support thick provisioning on the storage controller.</td>
                </tr>
            </tbody>
        </table>
        <caution>
            <para>
                If both the positive and negative specs for a pair are specified (e.g. <option>netapp_dedup</option> and <option>netapp_nodedup</option>) and set to the same value within a single extra_specs list, then neither spec will be utilized by the driver.
            </para>
        </caution>
        <important>
            <title>Recommendation</title>
            <para>
                NetApp recommends that none of the Boolean extra specs in <xref linkend="cinder.netapp.extra_specs"/> be configured with a value of <option>False</option>; rather only positive assertions (e.g. <option>netapp_nocompression=True</option>) of the option with the negated meaning be used.
            </para>
        </important>
    </simplesect>
</section>
